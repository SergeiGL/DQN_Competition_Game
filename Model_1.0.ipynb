{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 22:54:24.208125: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-06-25 22:54:24.677137: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-06-25 22:54:24.684097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-25 22:54:24.684162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.725GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2023-06-25 22:54:24.684171: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-06-25 22:54:24.686094: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-06-25 22:54:24.686121: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-06-25 22:54:24.700114: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-25 22:54:24.700253: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-25 22:54:24.715703: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-06-25 22:54:24.717025: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-06-25 22:54:24.717103: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-06-25 22:54:24.717157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-25 22:54:24.717232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-25 22:54:24.717269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-06-25 22:54:24.717484: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-25 22:54:24.717902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-25 22:54:24.717958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.725GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2023-06-25 22:54:24.717978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-25 22:54:24.718018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-25 22:54:24.718049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-06-25 22:54:24.718064: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-06-25 22:54:24.990832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-06-25 22:54:24.990851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2023-06-25 22:54:24.990854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2023-06-25 22:54:24.990947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-25 22:54:24.991021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-25 22:54:24.991067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-25 22:54:24.991116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21755 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 is done!\n",
      "Epoch 2 is done!\n",
      "Epoch 3 is done!\n",
      "Epoch 4 is done!\n",
      "Epoch 5 is done!\n",
      "Epoch 6 is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 22:54:25.250428: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-06-25 22:54:25.250629: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3609600000 Hz\n",
      "2023-06-25 22:54:25.284327: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-06-25 22:54:25.770785: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-06-25 22:54:25.770822: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "#import sys\n",
    "#import psutil\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, n_players=5):\n",
    "        self.n_players = n_players\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.players_position = [0 for _ in range(self.n_players)]\n",
    "        self.players_score = [0 for _ in range(self.n_players)]\n",
    "        self.stage_rewards = [1, 2, 13]\n",
    "        self.stage_capacity = [self.n_players, 3, 1]\n",
    "        self.max_moves = 10\n",
    "        self.number_of_pushes = 0\n",
    "\n",
    "    def get_state(self):\n",
    "        state = {\n",
    "            'players_position': self.players_position,\n",
    "            'players_score': self.players_score,\n",
    "            'time_left': self.max_moves\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    def player_step(self, action=0, player_index=0):\n",
    "        # actions: 0 - stay, 1 - up\n",
    "        if action == 0:\n",
    "            pass\n",
    "        else:\n",
    "            if self.players_position[player_index] != len(self.stage_rewards)-1:\n",
    "                my_stage = self.players_position[player_index] + 1\n",
    "                on_stage_players = np.where(np.array(self.players_position) == my_stage)[0]\n",
    "                # print('on_stage_players', on_stage_players, my_stage, self.stage_capacity[my_stage])\n",
    "                if len(on_stage_players) > self.stage_capacity[my_stage]-1:\n",
    "                    self.number_of_pushes+=1\n",
    "                    whom_to_push = np.random.choice(on_stage_players)\n",
    "                    # print('push', whom_to_push+1, ', I am', player_index+1)\n",
    "                    self.players_position[whom_to_push] = 0\n",
    "                self.players_position[player_index] += 1\n",
    "\n",
    "    def step(self, actions_list):\n",
    "        for i, action in enumerate(actions_list):\n",
    "                self.player_step(action, player_index=i)\n",
    "        \n",
    "        reward = [self.stage_rewards[self.players_position[i]] for i in range(len(self.players_position))]\n",
    "        self.players_score =  np.array(self.players_score)+np.array(reward)\n",
    "            \n",
    "        self.max_moves -= 1\n",
    "        is_game_end = False\n",
    "        if self.max_moves == 0:\n",
    "            is_game_end = True\n",
    "\n",
    "        state = self.get_state()\n",
    "\n",
    "        return state, reward, is_game_end\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=50_000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(32, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam())\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = np.zeros((batch_size, self.state_size))\n",
    "        targets = np.zeros((batch_size, self.action_size))\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            states[i] = state\n",
    "            target = self.model.predict(state, verbose=0)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.model.predict(next_state, verbose=0)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            targets[i] = target\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "# Initialize game and agents\n",
    "n_players = 6\n",
    "game = Game(n_players)\n",
    "state_size = len(game.get_state()['players_position']) + len(game.get_state()['players_score']) + 1  # +1 for 'time_left'\n",
    "action_size = 2\n",
    "agents = [DQNAgent(state_size, action_size) for _ in range(n_players)]\n",
    "batch_size = 64\n",
    "epochs = 3500\n",
    "\n",
    "dumb_filename = f'n_players-{n_players}epochs-{epochs}ledderlen-{len(game.stage_rewards)}movespergame-{game.max_moves}.txt'\n",
    "with open(f\"{dumb_filename}\", \"w\") as f:\n",
    "    f.write(f'stage_rewards = {game.stage_rewards}\\n')\n",
    "    f.write(f'stage_capacity = {game.stage_capacity}\\n')\n",
    "\n",
    "# Train the agents\n",
    "for epoch in range(epochs):\n",
    "    game.reset()\n",
    "    state = game.get_state()\n",
    "    state_values = np.concatenate([np.array(state['players_position']), \n",
    "                            np.array(state['players_score']), \n",
    "                            np.array([state['time_left']])])\n",
    "    state = np.reshape(state_values, [1, state_size])\n",
    "    while True:\n",
    "        actions = [agent.act(state) for agent in agents]\n",
    "        next_state, reward, done = game.step(actions)\n",
    "        next_state = np.concatenate([np.array(next_state['players_position']), \n",
    "                            np.array(next_state['players_score']), \n",
    "                            np.array([next_state['time_left']])])\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        #print(state, actions, reward, next_state, done)\n",
    "        for i, agent in enumerate(agents):\n",
    "            agent.remember(state, actions[i], reward[i], next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        if epoch%25==0:\n",
    "                with open(f\"{dumb_filename}\", \"a\") as f:\n",
    "                    f.write(f'{game.get_state()}, number_of_pushes = {game.number_of_pushes}\\n')\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    for agent in agents:\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "\n",
    "    #print(psutil.virtual_memory()[3] / 1000000000, 'Gb')\n",
    "    print(f'Epoch {epoch+1} is done!')\n",
    "\n",
    "print(\"Finished Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, agent in enumerate(agents):\n",
    "    agent.model.save_weights(f\"3500_model_w.a{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = agents[0].model.to_json()\n",
    "with open(\"3500_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy==1.19.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
